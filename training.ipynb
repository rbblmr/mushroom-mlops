{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipykernel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Clean data\n",
    "# Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical\n",
    "\n",
    "# Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(dataset):\n",
    "\n",
    "  def __init__(self, dataset):\n",
    "    self.dataset = dataset\n",
    "    self.num_features = num_features\n",
    "    self.cat_features = cat_features\n",
    "\n",
    "  def clean(self)\n",
    "    return self.X, self.y\n",
    "  def scale(self):\n",
    "    \n",
    "\n",
    "  def split(self, test_size):\n",
    "   X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=test_size, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeller(dict(X_train, y_train, X_test, y_test)):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.X\n",
    "    self.y\n",
    "  \n",
    "  def batch_model(models, X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame):\n",
    "  \"\"\"\n",
    "  models: Dictionary\n",
    "  X_train, y_train, X_test, y_test: Dataframes\n",
    "  \"\"\"\n",
    "\n",
    "  all_scores = {}\n",
    "  for model_name, regressor in tqdm(models.items()):\n",
    "\n",
    "      model = regressor\n",
    "\n",
    "      # Measure the time it takes to fit the model\n",
    "      start_time_fit = time.time()\n",
    "      model.fit(X_train, y_train)\n",
    "      end_time_fit = time.time()\n",
    "      fit_time = end_time_fit - start_time_fit\n",
    "\n",
    "      # Train pred\n",
    "      y_train_pred = model.predict(X_train)\n",
    "\n",
    "      # Test pred\n",
    "      y_test_pred = model.predict(X_test)\n",
    "\n",
    "      # Adjusted R2\n",
    "      adjusted_r2_train=adjusted_r2_score(y_train, y_train_pred, X_train)\n",
    "      adjusted_r2_test=adjusted_r2_score(y_test, y_test_pred, X_test)\n",
    "\n",
    "      scoring = {\n",
    "          'mae': mean_absolute_error,\n",
    "          'mape': mean_absolute_percentage_error,\n",
    "          'mse': mean_squared_error,\n",
    "          'rmse': lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "          'r2': r2_score,\n",
    "      }\n",
    "\n",
    "      all_scores[model_name] = {\n",
    "          'fit_time': fit_time,\n",
    "          'train_adjusted_r2': adjusted_r2_train,\n",
    "          'test_adjusted_r2': adjusted_r2_test,\n",
    "\n",
    "      }\n",
    "\n",
    "      for metric_name, scoring_function in scoring.items():\n",
    "          # Use the appropriate scoring function to calculate the score on the training set\n",
    "          train_score = scoring_function(y_train, y_train_pred)\n",
    "          all_scores[model_name][f'train_{metric_name}'] = train_score\n",
    "\n",
    "          # Use the appropriate scoring function to calculate the score on the test set\n",
    "          test_score = scoring_function(y_test, y_test_pred)\n",
    "          all_scores[model_name][f'test_{metric_name}'] = test_score\n",
    "\n",
    "  def hypertune(self, model_class, params):\n",
    "    all_scores={}\n",
    "    def search_fn(params):\n",
    "        with mlflow.start_run():\n",
    "            mlflow.set_tag('person','frankie') # Change name\n",
    "            mlflow.set_tag('model','xgb')\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            start_time_fit = time.time()\n",
    "            model=model_class(**params) # Change the model here\n",
    "            model.fit(X_train_full_cat_int, y_train_full_cat_int)\n",
    "            end_time_fit = time.time()\n",
    "            fit_time = end_time_fit - start_time_fit\n",
    "\n",
    "            # Train pred\n",
    "            y_train_final_pred = model.predict(X_train_full_cat_int)\n",
    "\n",
    "            # Test pred\n",
    "            y_test_final_pred = model.predict(X_test_full_cat_int)\n",
    "\n",
    "            scoring = {\n",
    "                'mae': mean_absolute_error,\n",
    "                'mape': mean_absolute_percentage_error,\n",
    "                'mse': mean_squared_error,\n",
    "                'rmse': lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "                'r2': r2_score,\n",
    "            }\n",
    "\n",
    "            all_scores['xgb'] = {\n",
    "                'fit_time': fit_time\n",
    "            }\n",
    "\n",
    "            for metric_name, scoring_function in scoring.items():\n",
    "                # Use the appropriate scoring function to calculate the score on the training set\n",
    "                train_score = scoring_function(y_train_full_cat_int, y_train_final_pred)\n",
    "                all_scores['xgb'][f'train_{metric_name}'] = train_score\n",
    "\n",
    "                # Use the appropriate scoring function to calculate the score on the test set\n",
    "                test_score = scoring_function(y_test_full_cat_int, y_test_final_pred)\n",
    "                all_scores['xgb'][f'test_{metric_name}'] = test_score\n",
    "\n",
    "            scores_df=pd.DataFrame(all_scores.values(), index=all_scores.keys())\n",
    "\n",
    "\n",
    "            mlflow.log_metrics(all_scores['xgb'])\n",
    "\n",
    "            return {'loss': all_scores['xgb']['test_mae'], 'status': STATUS_OK}\n",
    "\n",
    "    search_space = {\n",
    "        'n_estimators': scope.int(hp.uniform('n_estimators', 100, 500)),\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 10, 50, 1)),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -7, -2), # exp(-3), exp(0)\n",
    "        'min_child_weight': hp.loguniform('min_child_weight', -1, 5),\n",
    "        'objective': 'reg:absoluteerror',\n",
    "        'random_state': 42,\n",
    "        'booster': 'gbtree'\n",
    "    }\n",
    "    trials=Trials()\n",
    "    best_result = fmin(\n",
    "        fn=search_fn,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=30,\n",
    "        trials=trials\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
